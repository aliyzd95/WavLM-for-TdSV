# Fine-Tuning WavLM for Text-Dependent Speaker Verification

This project demonstrates how to fine-tune the WavLM model for **text-dependent speaker verification (TDSV)** using the DeepMine dataset. It focuses on adapting a pretrained WavLM model to a TDSV task by training it with phrase-level speaker-labeled utterances. This is not a complete end-to-end speaker verification system, but a fine-tuning setup tailored for research and evaluation purposes.

The fine-tuned model is publicly available here: [aliyzd95/wavlm-deepmine-base-plus-sv](https://huggingface.co/aliyzd95/wavlm-deepmine-base-plus-sv)

## ğŸ” Project Objective
- Fine-tune `wavlm-base-plus` specifically for **phrase-based speaker verification** (text-dependent).
- Adapt WavLM to handle speaker-specific variations across 10 predefined phrases.
- Evaluate speaker verification performance using EER (Equal Error Rate).

## ğŸ“‚ Dataset: DeepMine
The DeepMine dataset is a Persian-English speech database collected through crowdsourcing. Users installed an Android application and recorded designated phrases in real-world environments across Iran. It includes both text-dependent and text-independent utterances.

- **Language**: Primarily Persian (Farsi), with English phrases
- **Collection Environment**: Real-life noise conditions
- **Utterance Types**:
  - 5 fixed Persian + 5 fixed English phrases
  - 
### ğŸ”  Phrase Mapping (Text-Dependent Samples)
```python
phrase_mapping = {
    "01": "ØµØ¯Ø§ÛŒ Ù…Ù† Ù†Ø´Ø§Ù† Ø¯Ù‡Ù†Ø¯Ù‡ Ù‡ÙˆÛŒØª Ù…Ù† Ø§Ø³Øª.",
    "02": "ØµØ¯Ø§ÛŒ Ù‡Ø± Ú©Ø³ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯ Ø§Ø³Øª.",
    "03": "Ù‡ÙˆÛŒØª Ù…Ù† Ø±Ø§ Ø¨Ø§ ØµØ¯Ø§ÛŒ Ù…Ù† ØªØ§ÛŒÛŒØ¯ Ú©Ù†.",
    "04": "ØµØ¯Ø§ÛŒ Ù…Ù† Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ù…Ù† Ø§Ø³Øª.",
    "05": "Ø¨Ù†ÛŒ Ø¢Ø¯Ù… Ø§Ø¹Ø¶Ø§ÛŒ ÛŒÚ©Ø¯ÛŒÚ¯Ø±Ù†Ø¯.",
    "06": "My voice is my password.",
    "07": "OK Google.",
    "08": "Artificial intelligence is for real.",
    "09": "Actions speak louder than words.",
    "10": "There is no such thing as a free lunch.",
    "FT": "Free-Text"
}
```


## ğŸš€ Implementation Summary
- Framework: Hugging Face `transformers`, `datasets`, `torchaudio`, `librosa`
- Model: `WavLMForXVector` adapted for text-dependent verification
- Training: Hugging Face `Trainer` API
- Output: Cosine-similarity-based scoring for speaker verification

## ğŸ“‰ Loss Function: Additive Margin Softmax (AM-Softmax)
**AM-Softmax** introduces a margin in cosine similarity space to improve speaker separation. It penalizes intra-class variation and rewards inter-class margins, making embeddings more discriminative â€” ideal for verification tasks.

## ğŸ“Š Evaluation Metric: Equal Error Rate (EER)
**EER** measures the operating point where the false acceptance rate (FAR) equals the false rejection rate (FRR). A lower EER indicates better system performance. We use cosine similarity between speaker embeddings for scoring.

## ğŸ§  Training Configuration
- Mixed precision (`fp16`) for speed and memory efficiency
- Gradient checkpointing + accumulation
- Cosine margin loss with adaptive scale
- Phrase-level batching and evaluation
---
Feel free to explore the notebook and pretrained model to build or adapt your own speaker verification research!
